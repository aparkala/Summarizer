 More than 50% of customers create written shopping lists for major shopping visits [6]. Stores tend to use structured formats, e.g., product-category hierarchies (or taxonomies), that contain formal language, whereas customers tend to use a more free form natural language to describe items. As an example, a typical handwritten grocery list can contain everything from generic item descriptions (e.g., milk, juice) to very specific items (e.g., a specific package of washing powder).

The discrepancy between the way people create shopping lists and the way stores maintain product information raises the question of how to design an easy to use, intelligent mobile shopping assistant. While some design suggestions have been given in the literature, relatively little attention has been paid to the actual creation and use of shopping lists.

We describe and evaluate a grocery product retrieval system that can be used to map products expressed in natural language into products in a grocery store. Our system has been built using nine months of anonymized shopping data from a large Finnish supermarket (more than 12 million products bought). As the data is from a Finnish store, also the retrieval operates in Finnish. However, most of the techniques we employ are common information retrieval techniques, which suggests that our approach could also be used with other languages.

In addition to the shopping data, we have collected 132 real shopping lists from customers. Out of the lists, 62 were used to derive requirements for our retrieval system. The remaining 70 shopping lists were used to evaluate our system.

In the following we briefly describe the main functionality
of our retrieval system.
1. Indexing
The retrieval system was constructed using nine months of shopping basket data. From the data, we constructed a database of product and category names, as well as indexes for words occurring in product and category names. We also stemmed the words and created indexes for stems appearing in product and category names. Before creating the indexes, units and package sizes were removed from product and category names. Stemming was performed using the Snowball Finnish language stemmer.
2. Index Expansion
In order to handle compound words, we use prefix and suffix index expansion. More specifically, we expand the index of every word with product and category names that have a word that starts or ends with the word. For example, we add the product name Vipset appelsiinit¨aysmehu (Vipset orange juice) to the index of worr
d appelsiini (orange) since appelsiini is a prefix of appelsiinit¨aysmehu.
3. Query Preprocessing
We lemmatize queries by removing partitive (singular and plural) and nominative plural case endings.  In order to handle colloquialisms, slang expressions and abbreviations, we constructed a lookup table. For example, omppu is a colloquialism that is mapped to omena (apple).  We expand queries with additional words when a query word is extremely common (e.g., cheese) or when the category name is a compound word that does not occur in any of the product names of the products in the corresponding category (e.g., ananass¨ailyke (tinned pineapple, category))
 vs.  ananasmurska, (mashed pineapple, product)).  Words that span several product categories are considered stop words as they can cause the search to return a large number of potentially irrelevant products. Examples of stop words include manufacturer names and generic adjectives that describe characteristics of products.
4. Ranking
Items are ranked using an approximation of the posterior probability of an item given the query .
The variables ® and b are predefined constants. In the experiments we use Xapiaa
n2 default values ® = 1 and b = 0.5.  The variable fj is the term frequency of  
word j, nj is the number of product names in which term j appears, and N is the total number of product names. Finally, L is the normalized product name length, i.e., the length of the current product name divided by the average length of a product name. The variable¸ was set to 0.75 on the basis of preliminary experii
ments.
5. Rank Augmentation
Intuitively, products that match both in product name and category name should receive higher ranks than other products.  To achieve this behavior, we use a weighted extension of BM25 where the term and document frequencies are replaced with weighted linear sums. Here d, vj and wj are weight terms. In the experiments, we use d = 2 and the weights vj and wj are set using edit distance calculations.  We also use package sizes and stop words to augment rank scores. When the query contains a package size, we multiply the BM25 scores of matching products by d. If the product name matches a stop word that is part of the original query, we add the BM25 score of the stop word to the rank score.
6. Misspellings
To support misspellings, we maintain a distance index that contains word pairs and the edit distance between the two words. When the original query does not return any results, we consult the distance index using words within edit distance one. If this query does not return enough (ten or more) results, we use words within edit distance two.
6. Experiment Setting
In the experiment, we input the shopping lists into our search engine, one list at a time. The ten topmost results for each item in the shopping list were shown to an external evaluator who was asked to give positive or negative feedback on each result. The evaluator was also allowed to leave an item unrated if (s)he was not sure of the result. Each evaluator gave feedback on five shopping lists. The evaluators were recruited from students and staff of our department.
7. Results
The experiment resulted in a total of 7454 (subjective) relevance assessments. From the results, we calculated the mean average precision (MAP), as well as precision measures at different ranks. The resulting precision measures for the top 5 results are shown in Table 1.  We examined separately those queries whose top result was evaluated negatively. The main sources of negative feedback were (1) slang expressions that were not included in our lookup table, and (2) some complex queries where the product name was misspelled (e.g., query ottermanni 17%, result Valio Oltermanni 1kg ?~@~S this was rated negatively because the package did not match). These two types accounted for 23 cases of negative feedback and when ignoring these assessments, precision at one equals 84.3%. Hence, the performance of our system could be increased by collecting more slang expressions, colloquialisms, abbreviations etc.
